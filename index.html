<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SOLAR: Switchable Output Layer in Once-for-All Training</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="style.css">

    <style>
        body { font-family: Arial, sans-serif; margin:0; padding:0; line-height:1.6; background:#E3E3E3; }
        header { text-align:center; padding:60px 20px; background:#1B3C53; color:white; }
        h1 { font-size:42px; margin-bottom:10px; }
        h2 { border-bottom:2px solid #456882; padding-bottom:5px; }
        .authors { font-size:18px; margin-top:30px; }
        .container { max-width:1000px; margin:auto; padding:30px 20px; background:white; }
        .section { margin-bottom:50px; }
        .btn { display:inline-block; padding:10px 20px; background:#0077ff; color:white; border-radius:6px; margin:5px; text-decoration:none; }
        .btn:hover { background:#0059c9; }
        img { max-width:100%; border-radius:8px; margin:15px 0; }
        footer { text-align:center; padding:20px; background:#111; color:white; margin-top:40px; }
        .heading{
            color: #1B3C53;
        }
        
        .pretty-link {
            color: #0077b5;              /* text color */
            text-decoration: none;    /* remove underline */
            font-weight: 500;
            transition: 0.3s ease;
        }

        .pretty-link:hover {
        color: #a59595;        /* LinkedIn blue on hover */
        text-decoration: underline;
        }
        .para{
            text-align: justify;
            color: #4c4545;
        }
        .note{background:#fffbe6;border-left:4px solid #ffd24d;padding:12px;border-radius:6px;margin:12px 0;}
        .code-block{background:#0b0b0b;color:#e6e6e6;padding:12px;border-radius:6px;overflow:auto;}
        .muted{color:var(--muted); font-size:14px;}
        .small{font-size:13px;}


    </style>
</head>

<body>

<header>
    <h1>SOLAR</h1>
    <h2>Switchable Output Layer for Accuracy and Robustness in Once-for-All Training</h2>

    <div class="authors">
        <p>
            <b>
                <a href="https://www.linkedin.com/in/stareen/" class="pretty-link" target="_blank" rel="noopener noreferrer">
                Shaharyar Ahmed Khan Tareen  
                </a>
            </b>&nbsp;
                <a href="https://www.linkedin.com/in/lei-fan-b1857a23/" class="pretty-link" target="_blank" rel="noopener noreferrer">
                Lei Fan   
                </a>&nbsp;
                <a href="https://www.linkedin.com/in/xiaojing-yuan-36970232/" class="pretty-link" target="_blank" rel="noopener noreferrer">
                Xiaojing Yuan   
                </a>&nbsp;
                <a href="https://www.linkedin.com/in/qin-lin-6a104861" class="pretty-link" target="_blank" rel="noopener noreferrer">
                Qin Lin  
                </a>&nbsp;
                <a href="https://www.linkedin.com/in/bin-hu-a338902b/" class="pretty-link" target="_blank" rel="noopener noreferrer">
                Bin Hu
                </a></p>
        <p>University of Houston<br> Houston, TX, USA</p>
    </div>

    <div>
        <a class="btn" href="https://arxiv.org/pdf/2509.16833">ðŸ“„ Paper (PDF)</a>
        <a class="btn" href="https://arxiv.org/abs/2509.16833">arXiv</a>
        <a class="btn" href="https://github.com/sakt90/SOLAR">ðŸ’» Code</a>
    </div>
</header>


<div class="container">

    <!-- ABSTRACT -->
    <div class="section">
        <h2 class="heading">Abstract</h2>
        <p class="para">
        Once-for-All (OFA) training enables a single super-net
        to generate multiple sub-nets tailored to diverse deployment scenarios, 
        supporting flexible trade-offs among accuracy, robustness, and model-size 
        without retraining. However, as the number of supported sub-nets increases, 
        excessive parameter sharing in the backbone limits representational capacity, 
        leading to degraded calibration and
        reduced overall performance. To address this, we propose <b>SOLAR (Switchable 
        Output Layer for Accuracy and
        Robustness in Once-for-All Training)</b>, a simple yet effective
        technique that assigns each sub-net a separate classification head. By decoupling the logit learning process across
        sub-nets, the Switchable Output Layer (SOL) reduces representational interference and improves optimization, 
        without altering the shared backbone.<br><br> We evaluate SOLAR on five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and
        TinyImageNet) using four super-net backbones (ResNet-34,
        WideResNet-16-8, WideResNet-40-2, and MobileNetV2) for
        two OFA training frameworks (OATS and SNNs). Experiments show that
         SOLAR outperforms the baseline methods: compared to OATS, it improves accuracy of sub-nets
        up to <b>1.26%, 4.71%, 1.67%, and 1.76%</b>, and robustness up
        to <b>9.01%, 7.71%, 2.72%, and 1.26% </b>on SVHN, CIFAR-10,
        STL-10, and CIFAR-100, respectively. Compared to SNNs,
        it improves TinyImageNet accuracy by up to 2.93%, 2.34%,
        and 1.35% using ResNet-34, WideResNet-16-8, and MobileNetV2 backbones (with 8 sub-nets), respectively.

        </p>
    </div>


   <!-- Motivation -->
  <section class="card">
    <h2 class="heading">Motivation</h2>
    <p class="para">
      OFA frameworks (example: SNNs, OATS) successfully decouple intermediate statistics using switchable batch normalization (SBN / SDBN).
      However, the usual design still uses a <strong>single, shared output layer</strong> (classification head) for all sub-nets.
      Sub-nets of different widths produce feature distributions with very different statistics; forcing them to share the same logit head
      causes conflicting gradients and representational interference. This becomes more severe as the number of sub-nets grows.
    </p>

    <div class="note">
      <strong>Consequence:</strong> smaller sub-nets or those with different capacities are unable to learn sub-net specific output representations,
      which degrades accuracy, calibration, and robustness.
    </div>

    <img src="static/images/output-interference.PNG"
     alt="Shared output layer interference visualization"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para" style="text-align:center">
    <b>Figure.1</b> Shared output layer forces incompatible sub-net feature distributions into one logit space.
    </p>

  </section>

  <!-- What SOLAR does -->
  <section class="card" id="method">
    <h2 class="heading">Method â€” Switchable Output Layer (SOL)</h2>

    <h3>Core Idea</h3>
    <p class="para">
      Instead of using one shared classification head C for all sub-nets, SOL adds one head per sub-net: <code>C_{Î±k}</code>.
      When training / evaluating a sub-net with width multiplier <code>Î±k</code>, the model uses only <code>C_{Î±k}</code>.
      During backpropagation only the active head is updated. The shared encoder (backbone) remains shared.
    </p>

    
     <img src="static/images/method.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para" style="text-align:center">
     <b>Figure.2</b> OATS framework with Switchable Dual Batch Norm (SDBN) layers and Switchable Output Layer (SOL).
     Value of Î± indicates the width fraction for the corresponding sub-net in the backbone that is activated during training or inference.
     Convolutional layers have shared parameters whereas SDBN and SOL have separate parameters for each sub-net.
    </p>

    <h3>Formalization â€” Logits & Loss</h3>
    <p class="para">
      For sub-net S<sub>Î±k</sub> we compute features <code>f<sub>Î±k</sub> = S<sub>Î±k</sub>(x)</code>, and logits:
    </p>
    <img src="static/images/equ1.PNG"
     style="margin:14px 0; border-radius:8px;">

    <p>
      Standard cross-entropy:
    </p>
      <img src="static/images/equ2.PNG"
     style="margin:14px 0; border-radius:8px;">
    <pre><code>where p = softmax(z)</code></pre>

    <p>
      For hybrid adversarial training (OATS style), SOL supports the hybrid loss:
    </p>
    <img src="static/images/equ3.PNG"
     style="margin:14px 0; border-radius:8px;">

    <p>
      Here <code>L_ADV</code> is computed using PGD adversarial examples (e.g. 7-step PGD with &epsilon; = 8/255 as in the paper).
    </p>

    <h3>OATS-SOL Algorithm</h3>
    <img src="static/images/algo.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">


    <h3>Why SOLAR Outperforms?</h3>
    <ul class="para">
      <li>Decouples logit learning so each sub-net can adapt its output layer to its feature distribution.</li>
      <li>Reduces interference at the final layer, gradients no longer compete through a single shared head.</li>
      <li>Negligible inference overhead â€” only the active head is used per forward pass (FLOPs unchanged compared to baseline).</li>
    </ul>
  </section>

   <!-- Experiments -->
  <section class="card" id="experiments">
    <h2 class="heading">Experiments & Key Results</h2>

    <p>Datasets & Backbones used:</p>
    <ul class="para">
      <li>Datasets: SVHN, CIFAR-10, STL-10, CIFAR-100, TinyImageNet</li>
      <li>Backbones: WideResNet-16-8, ResNet-34, WideResNet-40-2, MobileNetV2</li>
      <li>OFA Frameworks augmented with SOL: OATS (Once-for-All Adversarial Training and Slimming) and SNNs (Slimmable Neural Networks)</li>
    </ul>

    <h3>Results</h3>    
    
     <img src="static/images/table 1.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
    <b>Figure.3</b> Comparison of OATS and OATS-SOL on SVHN dataset using WideResNet-16-8 backbone packed with 8 sub-nets.
    OATS-SOL provides superior performance than OATS for all the sub-nets in terms of accuracy and PGD-7 robustness.
    </p>

    <img src="static/images/table 2.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
    <b>Figure.4</b> Comparison of OATS and OATS-SOL on CIFAR-10 dataset using ResNet-34 backbone (with 8 sub-nets). OATS-SOL
    provides superior performance than OATS for all the sub-nets in terms of accuracy and PGD-7 based robustness.
    </p>

    <img src="static/images/table 3.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Figure.5</b> Comparison of OATS and OATS-SOL on STL-10 dataset using WideResNet-40-2 as backbone with 4 sub-nets.
    </p>

    <img src="static/images/table 4.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
    <b>Figure.6</b> Comparison of OATS and OATS-SOL on CIFAR-100 dataset using ResNet-34 as backbone with 4 sub-nets.
    </p>

    <img src="static/images/table 5.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
    <b>Table.1</b> Accuracy of SNN and SNN-SOL based Sub-Nets on CIFAR-10 for backbones packed with 16 Sub-Nets.
    </p>

    <img src="static/images/table 6.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Table.2</b> Accuracy of SNN and SNN-SOL on CIFAR-100 for backbones with 8 sub-nets.
    </p>

    <img src="static/images/table 7.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Table.3</b> Accuracy of SNN and SNN-SOL on TinyImageNet for backbones with 8 sub-nets.
    </p>
    
    <img src="static/images/table 8.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Table.4</b> Parameter counts of SNN vs. SNN-SOL backbones packed with 8, 32, and 64 sub-nets for CIFAR-10 dataset.
    </p>

    <img src="static/images/table 9.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Table.5</b> Identical FLOPs of SNN and SNN-SOL sub-nets
(with different widths), show that SOL adds no training overhead.
    </p>

    <img src="static/images/table 10.PNG"
     style="margin:14px auto; border-radius:8px; display:block;">
    <p class="para small" style="text-align:center">
     <b>Table.6</b> Performance of different sub-nets after fine-tuning.
    </p>

    <h3>Training & Evaluation Highlights:</h3>
    <ul class="para">
      <li>Adversarial training uses 7-step PGD (Lâˆž, &epsilon;=8/255, step=2/255) for robustness evaluation â€” same as OATS baseline.</li>
      <li>Hybrid Î» values used for training: sampled from S = {0.0, 0.1, 0.2, 0.3, 0.4, 1.0}.</li>
      <li>Optimization: SGD with momentum 0.9, cosine annealing LR; reported results are best-of-runs across seeds.</li>
    </ul>
  </section>

    <!-- Conclusion -->
  <section class="card" id="conclusion">
    <h2 class="heading">Conclusion & Future Directions</h2>
    <p class="para">
      We introduce <b>Switchable Output Layer (SOL)</b> to enhance
the performance and robustness of Once-for-All (OFA)
training frameworks. SOL assigns independent classification heads to the sub-nets in the super-net backbone,
which decouples their logit learning processes, mitigating
the competition at the shared output layerâ€”a bottleneck
limiting the sub-net accuracy, robustness, and optimization.
Extensive experiments on two different baseline methods:
Once-for-All Adversarial Training and Slimming (OATS)
and Slimmable Neural Networks (SNNs), across multiple
datasets and diverse super-net architectures, demonstrate
that incorporation of SOL consistently improves performance of sub-nets without introducing additional training
overhead or complexity. SOL generalizes well, which highlights its potential as an effective enhancement for the OFA
frameworks, encouraging flexible, scalable, and reliable deployment of specialized models across a wider range of
devices and constraints. For future, we aim to extend SOL to more OFA frameworks
and conduct large-scale evaluations on the ImageNet-1K
dataset. In addition, we intend to study the impact of
layer normalization on reducing representational interference across the sub-nets.
    </p>


<!-- Repro & citation -->
  <section>
    <h2>BibTeX</h2>
    <pre><code>@inproceedings{tareen2026solar
  title     = {SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training},
  author    = {Tareenâ˜…, Shaharyar Ahmed Khan and Fan, Lei and Yuan, Xiaojing and Lin, Qin and Hu, Bin},
  booktitle = {Proc. IEEE/CVF Winter Conf. on Applications of Computer Vision (Round 2 New Submission Acceptance Rate: 26.88%)},
  year      = {2026},
  address   = {Tucson, Arizona, USA},
}</code></pre>

<p>
    Website borrowed from NeRFies under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>

</body>
</html>
