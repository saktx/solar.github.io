<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>SOLAR: Switchable Output Layer in Once-for-All Training</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="style.css">

    <style>
        body { font-family: Arial, sans-serif; margin:0; padding:0; line-height:1.6; background:#fafafa; }
        header { text-align:center; padding:60px 20px; background:#111; color:white; }
        h1 { font-size:42px; margin-bottom:10px; }
        h2 { border-bottom:2px solid #ddd; padding-bottom:5px; }
        .authors { font-size:18px; margin-top:10px; }
        .container { max-width:900px; margin:auto; padding:30px 20px; background:white; }
        .section { margin-bottom:50px; }
        .btn { display:inline-block; padding:10px 20px; background:#0077ff; color:white; border-radius:6px; margin:5px; text-decoration:none; }
        .btn:hover { background:#0059c9; }
        img { max-width:100%; border-radius:8px; margin:15px 0; }
        footer { text-align:center; padding:20px; background:#111; color:white; margin-top:40px; }
    </style>
</head>

<body>

<header>
    <h1>SOLAR</h1>
    <h2>Switchable Output Layer for Accuracy and Robustness in Once-for-All Training</h2>

    <div class="authors">
        <p><b>Shaharyar Ahmed Khan Tareen</b>, Lei Fan, Xiaojing Yuan, Qin Lin, Bin Hu</p>
        <p>University of Houston</p>
    </div>

    <div>
        <a class="btn" href="2509.16833v1.pdf">ðŸ“„ Paper (PDF)</a>
        <a class="btn" href="https://arxiv.org/abs/2509.16833">arXiv</a>
        <a class="btn" href="https://github.com/sakt90/SOLAR">ðŸ’» Code</a>
    </div>
</header>


<div class="container">

    <!-- ABSTRACT -->
    <div class="section">
        <h2>Abstract</h2>
        <p>
        Once-for-All (OFA) training enables a single super-network to generate multiple sub-networks
        with different accuracy, robustness, and model-size trade-offs. However, when many sub-nets
        share the same output layer, representational interference occurs, harming performance and
        calibration.

        We propose <b>SOLAR (Switchable Output Layer)</b>, a simple but powerful modification that
        assigns each sub-net a separate classification head. SOLAR eliminates logit interference,
        improves optimization stability, and significantly boosts both accuracy and adversarial
        robustness across all evaluated datasets and backbones.
        </p>
    </div>


    <!-- MOTIVATION -->
    <div class="section">
        <h2>Motivation: The Output-Layer Bottleneck</h2>
        <p>
        Existing OFA frameworks such as OATS and SNNs use Switchable BatchNorm to separate
        intermediate feature statistics across widths. However, they still share a single
        classification layer, which forces incompatible representations into one output space.
        This results in degraded performance, especially for smaller sub-nets.
        </p>

        <img src="assets/bn_stats.png" alt="BN statistics visualization">
        <p style="text-align:center;">Shared output layer causes logit interference across sub-nets.</p>
    </div>


    <!-- METHOD -->
    <div class="section">
        <h2>Method: Switchable Output Layer (SOL)</h2>
        <p>
        SOLAR introduces a lightweight technique:
        </p>
        <ul>
            <li>Each sub-net receives its own classification head</li>
            <li>Only the active head is updated during training</li>
            <li>No additional FLOPs during inference</li>
            <li>Minimal parameter increase (<1â€“3% for most models)</li>
        </ul>

        <img src="assets/solar_method.png" alt="SOLAR architecture diagram">
        <p style="text-align:center;">SOL decouples logit learning for each sub-net.</p>
    </div>


    <!-- RESULTS -->
    <div class="section">
        <h2>Results</h2>
        <p>
        SOLAR consistently improves accuracy and PGD robustness across:
        </p>
        <ul>
            <li>SVHN, CIFAR-10, STL-10, CIFAR-100, TinyImageNet</li>
            <li>ResNet-34, WRN-16-8, WRN-40-2, MobileNetV2</li>
            <li>OATS and SNN frameworks</li>
        </ul>

        <img src="assets/svhn_results.png" alt="SVHN results">
        <img src="assets/cifar10_results.png" alt="CIFAR-10 results">
        <img src="assets/tinyimagenet_results.png" alt="TinyImageNet results">
    </div>


    <!-- CONCLUSION -->
    <div class="section">
        <h2>Conclusion</h2>
        <p>
        SOLAR resolves a long-standing bottleneck in OFA training by isolating sub-net output
        representations without increasing computational cost. Its simplicity, generality, and strong
        experimental results make it a practical upgrade for all OFA-based frameworks.
        </p>
    </div>

</div>


<footer>
    <p>Â© 2025 â€“ SOLAR Project | University of Houston</p>
</footer>

</body>
</html>
